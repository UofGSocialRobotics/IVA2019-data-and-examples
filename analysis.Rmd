---
title: "IVA2019 Analysis"
author: "CSaund"
date: "2/21/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(pwr)
```

AA = "available audience"
E = entity
U = unpolite
O = original
AAOP = AA open
AAS = AA Small
ES = E separate
EC = E control
UT = U together
UF = U force

## Load 'N' Wrangle

Load these bad larrys
Annoyingly can't create dynamically named variables in R AFAIK, so manual
```{r}
condition_names <- c('aao', 'aas', 'aaop', 'eo','ec','es','uo','uf','ut')
aao <- read.csv('AAO_copy.csv')
aas <- read.csv('AAS_copy.csv')
aaop <- read.csv('AAOP_copy.csv')
eo <- read.csv('EO_copy.csv')
ec <- read.csv('EC_copy.csv')
es <- read.csv('ES_copy.csv')
uo <- read.csv('UO_copy.csv')
ut <- read.csv('UT_copy.csv')
uf <- read.csv('UF_copy.csv')
```

First step, collapse responses into metaphor-y responses. 
1. (Force is conflict) Combine rows for "There is tension between people in the group" and "People in the group disagree with one another"
2. (Physical closeness is ideological closeness) Combine "People in this group generally get along" and "People in the group are working together to solve a problem"
3. (Abstract ideas have concrete properties (size)) "She is referring to everybody in the group" and "This group consists of many people"
4. (Open is accessible) "The speaker likes the people in the group" and "The speaker is open to feedback from the group"
5. (Being in control is being above) "The speaker is annoyed with the group" and "The speaker is in control of the group"

We add a "group"column that actually tells us what the metaphors were in reference to. 
```{r}
group_by_metaphor <- function(data) {
  data$metaphor_measure <- ""
  for(row in 1:nrow(data)) {
    if (data$Answer.Choices[row] == "People in the group are working together to solve a problem" || 
        data$Answer.Choices[row] == "People in the group generally get along") {
      data$metaphor_measure[row] <- "closeness"
    } else if (data$Answer.Choices[row] == "There is tension between people in the group" || 
               data$Answer.Choices[row] == "People in the group disagree with one another") {
      data$metaphor_measure[row] <- "conflict"
    } else if (data$Answer.Choices[row] == "She is referring to everybody in the group" || 
               data$Answer.Choices[row] == "This group consists of many people") {
      data$metaphor_measure[row] <- "size"
    } else if (data$Answer.Choices[row] == "The speaker likes the people in the group" || 
               data$Answer.Choices[row] == "The speaker is open to feedback from the group") {
      data$metaphor_measure[row] <- "open"
    } else if (data$Answer.Choices[row] == "The speaker is in annoyed with the group" || 
               data$Answer.Choices[row] == "The speaker is in control of the group") {
      data$metaphor_measure[row] <- "control"
    } else {
      # for some reason it's not recognizing the annoyed case, so let's throw it in the else
      # cause that seems safe.
      data$metaphor_measure[row] <- "control"
    }
  }
  return(data)
}
```

This means we're gonna have to calculate our own scores
```{r}
# Get all the overall scores (mean for condition) for a dataset
get_scores <- function(data) {
  scores <- c()
  for(row in 1:nrow(data)) {
    total_score = 0
    for(col in 2:11) {
      total_score = total_score + data[row,col] * (12 - col)
    }
    scores[row] <- (total_score / data[row,12])
  }
  return(scores)
}

# variances while we're at it too
get_variances <- function(data) {
  sds <- c()
  for(row in 1:nrow(data)) {
    variances <- c()
    for(col in 2:11) {
      variances <- append(variances, data[[row, col]])
    }
    sds[row] <- sd(variances)
  }
  return(sds)
}

# actually add the scores to the dataframe
append_scores <- function(data) {
  scores <- get_scores(data)
  variances <- get_variances(data)
  print(scores)
  print(variances)
  data$normalized_score=scores
  data$metaphor_variance=variances
  return(data)
}

# Aggregate the data for columns that have the 
# same metaphor measure
bucket_by_group <- function(data) {
  return (aggregate(list(X1=data$X1, 
                 X2=data$X2,
                 X3=data$X3,
                 X4=data$X4,
                 X5=data$X5,
                 X6=data$X6,
                 X7=data$X7,
                 X8=data$X8,
                 X9=data$X9,
                 X10=data$X10,
                 Total=data$Total), 
             by=list(metaphor_measure=data$metaphor_measure), 
             FUN=sum))
}
```
Sweet, now instead of dealing with survey statements we're dealing with semantic meaning. 

Just to be clear, this is what we're doing start to finish to get a nice and shiny 
bucketed dataset with new scores we can then compare based on the semantic
meanings and not the actual survey statements.
So let's transform all our old data so it's nice and bucketed, and sorted by score
```{r}
transform_data <- function(data) {
  transformed_data <- data %>%
    group_by_metaphor() %>%
    bucket_by_group() %>%
    append_scores() 
  return(transformed_data[rev(order(transformed_data$normalized_score)),])
}

aao <- transform_data(aao)
aas <- transform_data(aas)
aaop <- transform_data(aaop)

eo <- transform_data(eo)
ec <- transform_data(ec)
es <- transform_data(es)

uo <- transform_data(uo)
uf <- transform_data(uf)
ut <- transform_data(ut)
```


Let's do ourselves a favor and put these next to each other, so we have one table per
gesture, as opposed to 3. There is *for sure* a nicer way to do this but this is quick and dirty.
I'm sorry for what I've done.
```{r}
summarise_gesture <- function(orig, manip1, manip1_name, manip2, manip2_name) {
  # drop columns we don't care about
  keep = c("metaphor_measure", "normalized_score", "metaphor_variance")
  
  orig <- orig[keep] 
  colnames(orig)[2] <- "original"
  
  manip1 <- manip1[keep]
  colnames(manip1)[2] <- manip1_name
  
  manip2 <- manip2[keep]
  colnames(manip2)[2] <- manip2_name
  
  summary_table <- left_join(orig, manip1, by="metaphor_measure") %>%
    left_join(manip2, by="metaphor_measure")
}

aa_summary <- summarise_gesture(aao, 
                                aas, "small",
                                aaop, "open")
                                
entity_summary <- summarise_gesture(eo, 
                                es, "separate",
                                ec, "chest")
                                
unpolite_summary <- summarise_gesture(uo, 
                                ut, "together",
                                uf, "force")
```


Now we have nice summaries of the data, but let's make sure we have a big table too.
```{r}
scale_scores <- function(data) {
  data$X1 <- data$X1 * 10
  data$X2 <- data$X2 * 9
  data$X3 <- data$X3 * 8
  data$X4 <- data$X4 * 7
  data$X5 <- data$X5 * 6
  data$X6 <- data$X6 * 5
  data$X7 <- data$X7 * 4
  data$X8 <- data$X8 * 3
  data$X9 <- data$X9 * 2
  data$X10 <- data$X10 * 1
  return(data)
}

scale_gather_normalize <- function(data) {
  data <- scale_scores(data) %>%
    gather("ranking_position", "scaled_score", 2:11)
  data$normalized_scores <- data$scaled_score / data$Total
  return(data)
}

scale_and_normalize <- function(orig_data, cond1_data, cond1_name, cond2_data, cond2_name) {
  # scale all the scores
  orig_data <- scale_gather_normalize(orig_data) 
  cond1_data <- scale_gather_normalize(cond1_data) 
  cond2_data <- scale_gather_normalize(cond2_data) 
  
  # name it up nice
  orig_data$condition <- "original"
  cond1_data$condition <- cond1_name
  cond2_data$condition <- cond2_name
  
  #concat the datasets
  return(rbind(orig_data, cond1_data, cond2_data))
}

 total_unpolite <- scale_and_normalize(uo, ut, "together", uf, "forced")
 total_entity <- scale_and_normalize(eo, ec, "chest", es, "separated")
 total_audience <- scale_and_normalize(aao, aas, "small", aaop, "open")
```

## Plot 'Em and Load 'Em

Our "outliers" are actually the most important part of our data, so let's make sure we visualize them as such:
```{r}
# define the summary function to describe our stats the way we want
f <- function(x) {
  r <- quantile(x, probs = c(0.25, 0.5, 0.75, 0.999, 0.99999))
  names(r) <- c("ymin", "lower", "middle", "upper", "ymax")
  r
}

```

And now let's graph if up real style 
```{r}
plot_the_thing <- function(data) {
  data_plot <- ggplot(data, aes(x=metaphor_measure, y=normalized_scores, fill=condition)) + 
  stat_summary(fun.data=f, geom="boxplot", position="dodge2") + 
  facet_wrap(~metaphor_measure, scale="free")
  
  return(data_plot)
}

unpolite_plot <- plot_the_thing(total_unpolite)
audience_plot <- plot_the_thing(total_audience)
entity_plot <- plot_the_thing(total_entity)
```
Unpolite Plot:
```{r}
unpolite_plot
```
Audience Plot:
```{r}
audience_plot
```
Entity Plot
```{r}
entity_plot
```

Great, that gives a better picture of everything together


## Stats on Stats on Stats
```{r}
transpose_df <- function(data, transpose_var="metaphor_measure") {
  n <- data$transpose_var
  data <- as.data.frame(t(data[,-1]))
  colnames(data) <- n
  data$transpose_var <- factor(row.names(data))
  return(data)
}
```

Let's check to see, for example, how much the variation in 'closeness' is dependent on our condition
```{r}
unpolite_closeness_aov <- aov(normalized_scores ~ condition, data=filter(total_unpolite, metaphor_measure=='closeness'))
summary(unpolite_closeness_aov)
```

Basically the answer is... not much. But we can try for all of the different conditions.
```{r}
generate_anovas <- function(data) {
  print('closeness, conflict, control, open, size')
  print(summary(generate_anova(data, 'closeness')))
  print(summary(generate_anova(data, 'conflict')))
  print(summary(generate_anova(data, 'control')))
  print(summary(generate_anova(data,'open')))
  print(summary(generate_anova(data,'size')))
}

generate_anovas_by_mean <- function(data) {
  print('closeness, conflict, control, open, size')
  print(summary(generate_anova_by_mean(data, 'closeness')))
  print(summary(generate_anova_by_mean(data, 'conflict')))
  print(summary(generate_anova_by_mean(data, 'control')))
  print(summary(generate_anova_by_mean(data,'open')))
  print(summary(generate_anova_by_mean(data,'size')))
}

# not particularly useful up above, but handy for having during test period. 
generate_anova <- function(data, metaphor_filter) {
  return(aov(normalized_scores ~ condition, data=filter(data, metaphor_measure==metaphor_filter)))
}

generate_anova_by_mean <- function(data, metaphor_filter) {
  # normalized SCORE not normalized scoreS
  return(aov(normalized_score ~ condition, data=filter(data, metaphor_measure==metaphor_filter)))
}
```

```{r}
print('unpolite')
generate_anovas(total_unpolite)
```

```{r}
print('audience')
generate_anovas(total_audience)
```

```{r}
print('entity')
generate_anovas(total_entity)
```


###TODO: MANOVAS // compare between groups??


### Bootstrapping the ~Data~
Now let's try bootstrapping this bad boy
```{r}
generate_bootstrapped_data <- function(data, n) {
  generated_data <- data[0,]
  for(i in 1:n) {
    generated_data <- rbind(generated_data, sample_n(data, 1))
  }
  return(generated_data)
}
```


Perhaps fortunately, perhaps not, this brings significance values wayyyyy up. Let's compare:
```{r}
unpolite_bootstrapped <- generate_bootstrapped_data(total_unpolite, 150)
print('unpolite')
generate_anovas(unpolite_bootstrapped)
```

```{r}
audience_bootstrapped <- generate_bootstrapped_data(total_audience, 150)
print('audience')
generate_anovas(audience_bootstrapped)
```

```{r}
entity_bootstrapped <- generate_bootstrapped_data(total_entity, 150)
print('entity')
generate_anovas(entity_bootstrapped)
```



### Show Me What Matters
Fine, I will. 

```{r}
## Make it pretty
print_anova_significance <- function(data) {
  print('closeness')
  print(summary(generate_anova(data, 'closeness'))[[1]][["Pr(>F)"]][1])
  print('conflict')
  print(summary(generate_anova(data, 'conflict'))[[1]][["Pr(>F)"]][1])
  print('control')
  print(summary(generate_anova(data, 'control'))[[1]][["Pr(>F)"]][1])
  print('open')
  print(summary(generate_anova(data,'open'))[[1]][["Pr(>F)"]][1])
  print('size')
  print(summary(generate_anova(data,'size'))[[1]][["Pr(>F)"]][1])
}
```
Now we need to actually average the p values from bootstrapping
```{r}
## Do it a buncha times
average_p <- function(data, metaphor_measure, n) {
  avg_measure <- c()
  for(i in 1:n) {
    bootstrapped_data <- generate_bootstrapped_data(data, 150)
    bootstrapped_p <- summary(generate_anova(bootstrapped_data, metaphor_measure))[[1]][["Pr(>F)"]][1]
    avg_measure <- c(avg_measure, bootstrapped_p)
  }
  return(mean(avg_measure))
}

average_ps <- function(data, n) {
  print('closeness')
  print(average_p(data, 'closeness', n))
  print('control')
  print(average_p(data, 'control', n))
  print('size')
  print(average_p(data, 'size', n))
  print('open')
  print(average_p(data, 'open', n))
  print('conflict')
  print(average_p(data, 'conflict', n))
}
```


# "Unpolite"
```{r}
print_anova_significance(total_unpolite)
plot_the_thing(total_unpolite)
# average p values of BOOTSTRAPPED data
average_ps(total_unpolite, 100)
```


# "Available Audience"
```{r}
print_anova_significance(total_audience)
plot_the_thing(total_audience)
# average p values of BOOTSTRAPPED data
average_ps(total_audience, 100)
```

# "Entity"
```{r}
print_anova_significance(total_entity)
plot_the_thing(total_entity)
# average p values of BOOTSTRAPPED data
average_ps(total_entity, 100)
```


## Power Analysis
```{r}
#pwr.anova.test(k = , n = , f = , sig.level = , power = )
#where k is the number of groups and n is the common sample size in each group.
#For a one-way ANOVA effect size is measured by f where
# Cohen suggests that f values of 0.1, 0.25, and 0.4 represent small, medium, and large effect sizes respectively.
pwr.anova.test(k=3, n=20, sig.level=0.15, f=0.25)
```
Zoinks that ain't great. 


### Other Exploratory Analyses
#### 1. Just look at top three metaphors for each condition
```{r}
compare <- function(orig, manip1, manip2) {
  original <- orig$metaphor_measure[1:3]
  manip1 <- manip1$metaphor_measure[1:3]
  manip2 <- manip2$metaphor_measure[1:3]
  df <- data.frame(original, manip1, manip2)
  return(df)
}
```

#### 2. Look at variances of metaphors for each condition
we can do this by just adding the variance to each of our original datasets
```{r}
# playing with some graphs
ggplot(total_unpolite, aes(x=normalized_score, y=metaphor_variance, 
                           shape=condition, 
                           color=condition,
                           size=condition)) +
  geom_point()

ggplot(total_audience, aes(x=normalized_score, y=metaphor_variance, 
                           shape=condition, 
                           color=condition,
                           size=condition)) +
  geom_point()

ggplot(total_entity, aes(x=normalized_score, y=metaphor_variance, 
                           shape=condition, 
                           color=condition,
                           size=condition)) +
  geom_point()
```
Looks like there's really not much to it. Weak correlation at best.
Checking the rsq:
```{r}
rsq <- function(x, y) {
  cor(x, y)  ^ 2
}

rsq(total_unpolite$normalized_score, total_unpolite$metaphor_variance)
rsq(total_audience$normalized_score, total_audience$metaphor_variance)
rsq(total_entity$normalized_score, total_entity$metaphor_variance)

# But what about when we BOOTSTRAP IT
rsq_bootstrapped <- function(data, n) {
  avg_measure <- c()
  for(i in 1:n) {
    bootstrapped_data <- generate_bootstrapped_data(data, 15)
    bootstrapped_rsq <- rsq(data$normalized_score, data$metaphor_variance)
    avg_measure <- c(avg_measure, bootstrapped_rsq)
  }
  return(mean(avg_measure))  
}

rsq_bootstrapped(total_unpolite, 100)
rsq_bootstrapped(total_audience, 100)
rsq_bootstrapped(total_entity, 100)
```
lol terrible. 

#### 3. Bootstrap based on the means
```{}
# messy but bootstrapping based on means means we gotta do it
test_p_bootstrap_by_means <- function(data, metaphor_measure, n) {
  avg_measure <- c()
  for(i in 1:n) {
    bootstrapped_data <- generate_bootstrapped_data(data, 50)
    bootstrapped_p <- summary(generate_anova_by_mean(bootstrapped_data, metaphor_measure))[[1]][["Pr(>F)"]][1]
    avg_measure <- c(avg_measure, bootstrapped_p)
  }
  return(mean(avg_measure))  
}

#test_p_bootstrap_by_means(total_unpolite, "control", 100)
```
OK this doesn't work because we have such a small amount of data (means) that we're basically
guaranteed to sample all of them.




